openapi: 3.1.0
info:
  title: Open Inference API for text generation
  description: Open Inference API for text generation
  version: 1.0.0
components:
  schemas:
    GenerateRequest:
      type: object
      required: 
        - text_input
      properties:
        text_input:
          type: string
        parameters:
          $ref: '#/components/schemas/GenerateParameters'
    GenerateParameters:
      type: object
      properties:
        parameter:
          description: Parameters are framework specific. An optional object containing zero or more parameters for this generate request expressed as key/value pairs 
        temperature:
          type: number
          format: float
          default: null
          minimum: 0
          description: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        top_p:
          type: number
          format: float
          maximum: 1
          minimum: 0
          description: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        max_token:
          type: integer
          format: int32
          default: 20
          minimum: 0
          maximum: 512
          description: The maximum number of tokens to generate in the completion.
        best_of:
          type: integer
          format: int32
          minimum: 0
          description: Generates best_of completions server-side and returns the \best\ (the one with the highest log probability per token).
        stop:
          type: array
          items:
            type: string
          maxItems: 5
          description: Up to 5 sequences where the API will stop generating further tokens.
        frequency_penalty:
          type: number
          format: float
          minimum: 0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    GenerateResponse:
      type: object
      required:
        - text_output
        - model_name
        - model_version
      properties:
        text_output:
          type: string
        model_name:
          type: string
        model_version:
          type: string
    GenerateStreamResponse:
      type: object
      required:
        - text_output
        - model_name
        - model_version
        - done
      properties:
        text_output:
          type: string
        model_name:
          type: string
        model_version:
          type: string
        done:
          type: boolean
    GenerateErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          type: string
paths:
  /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/generate:
    post:
      parameters:
        - name: model_name
          required: true
          in: path
          schema:
            type: string
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
      responses:
        '200':
          description: generated text
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateResponse'
  /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/generate_stream:
    post:
      parameters:
        - name: model_name
          required: true
          in: path
          schema:
            type: string
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
      responses:
        '200':
          description: generated text stream
          content:
            text/event-stream:
              schema:
                $ref: '#/components/schemas/GenerateStreamResponse'
